# Server configuration
server.port=8081

# Configuration pour Ollama (chat)
spring.ai.ollama.chat.options.model=llama3.2
spring.ai.ollama.chat.options.temperature=1.4
spring.ai.ollama.chat.options.num-ctx=4096

# URL de l'API Ollama (peut-Ãªtre utile si tu as un appel HTTP direct)
ollama.url=http://localhost:11434/api/generate

# JPA Configuration
spring.jpa.generate-ddl = true
spring.jpa.hibernate.ddl-auto = update

# Application title and version, custom properties for demonstration
spring.application.name=FullStackIA
spring.profiles.active=dev
app.title=YnovIA
app.version=1.0.0